#+title: A_tour_of_machine_learning_algorithms

Source: [[https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/][Machine Learning Mastery]]

Two ways to think and categorized the algorithms you may come across in the field:
1. Grouping algorithms by their _*learning style*_.
2. Grouping algorithms by their _*similarity*_ in form of function.


* Algorithms Grouped by Learning Style

The three main learning styles in machine learning algorithms

1. _Supervised Learning_
   Input data is called training data and has a known label or result such as spam/not-spam or a stock price at a time.
   A model is prepared through a training process in which it is required to make predictions and is corrected whenthsoe predictions are wrong. The training process continues until the model achieves a desired level of accuracy on the training data.
   Examples problems: classification and regression.
   Example algorithms: Logistic regression and the Back Propagation Neural Network.
2. _Unsupervised Learning_
   Input data is not labeled and does not have a known result.
   A model is prepared by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to systematically reduce redunancy, or it may be to organize data by similarity.
   Examples  problems: clustering, dimensionality reduction and association rule learning
   Example algorithms include: the Apriori algorithm and K-means.
3. _Semi-supervised learning_
   Input data is a mixture of labeled and unlabelled examples.
   There is a desired prediction problem but the model must learn the structures to organize the data as well as make predictions.
   Example problems: classification and regression.
   Example algorithms: extensions to other flexible models that make assumptions about how to model the unlabelled data.
   Hot topic in areas such as image classification where there are a large datasets with very few labeled examples.


* Algorithms Grouped by Similarity

** Regression Algorithms
The most popular methods are:
- Ordinary Least Squares Regression (OLSR)
- Linear Regression
- Logistic Regression
- Stepwise Regression
- Multivariate Adaptive Regression Splines (MARS)
- Locally Estimated Scatterplot Smoothing (LOESS)
** Instance-based Algorithms
Instance-based learning model is decision problem with instances or examples of training data that are deemed important or required to the model.
Such methods build a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on the representation of the stored instances and similarity measures used between instances.
- k-Nearest Neighbor (kNN)
- Learning Vector Quantization (LVQ)
- Self-Organizing Map (SOM)
- Locally Weighted Learning (LWL)
- Support Vector Machines (SVM)
** Regularization Algorithms
Extension to other methods (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.

The most popular regularization algorithms are:
- Ridge Regression
- Least Absolute Shrinkage and Selection Operator (LASSO)
- Elastic Net
- Least-Angle Regression (LARS)

** Decision Tree Algorithms
Decision tree methods construct a model of decisions made based on actual values of attributes in the data. Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classficiation and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.

The most popular decision tree algorithms are:
- Classification and Regression (CART)
- Iterative Dichomotiser 3 (ID3)
- C4.5 and C5.0 (different versions of a powerful approach)
- Chi-squared Automatic Interaction Detection (CHAID)
- Decision Stump
- M5
- Conditional Decision Trees


** Bayesian Algorithms
Methods that explicity apply Bayes' Theorem for classification and regression problems.

The most popular Bayesian algorithms are:
- Naive Bayes
- Gaussian Naive Bayes
- Multinomial Naive Bayes
- Averaged One-Dependence Estimators (AODE)
- Bayesian Belief Network (BBN)
- Bayesian Network (BN)


** Clustering Algorithms
Clustering, like regressions, descrivves the class of problem and the class of methods.

There are two main modeling approaches for clustering: centroid-based and hierarchical. All methods use the inherent structures in the data to best organize the data into groups of maximum commonality.

The most popular clustering algorithms are:
- k-Means
- k-Medians
- Expectation Maximisation (EM)
- Hierachical Clustering


** Association Rule Learning Algorithms

These methdos extract rules that best explain observed relationships between variables in data. They discover important and commercially useful assocaitions in large multi-dimensional datasets that can be exploited by an organization.

The most popular association rule learning algorithms are:
- Apriori algorithm
- Eclat algorithm


** Artificial Neural Network Algorithms
ANN are models that are inspired by the structure and/or function of biological neural networks. They are a class of pattern matching, commonly used for regression and classification problems, but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.

The most popular ANN algorithms are:
- Perceptron
- Multilayer Perceptors (MLP)
- Back-Propagation
- Stochastic Gradient Descent
- Hopfield Network
- Radial Basis Function Network (RBFN)


** Deep Learning Algorithms
Deep learning methods are a modern update to ANN that explot cheap computation. They build larger and more complex neural networks, and many methods are concerned with very large datasets of labelled analog data, such as image, text, audio and video.

The most popular deep learning algorithms are:
- Convolutional Neural Network (CNN)
- Recurrent Neural Netorks (RNNs)
- Long Short-Term Memory Networks (LSTMs)
- Stacked Auto-Encoders
- Depep Boltzmann Machine (DBM)
- Deep Belief Networks (DBN)



** Dimensionality Reduction Algorithms
Similar to clustering methods, dimensionality reduction exploit the inherent structure in the data, but in this case in an unsupervised manner or in order to summarize or descrive data using less information.

Useful methods to visualize data or simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.

The most popular algorithms of this class are:
- Principal Component Aanalysis (PCA)
- Principal Component Regression (PCR)
- Partial Least Squares Regression (PLSR)
- Sammon Mapping
- Multidimensional Scaling (MDS)
- Projection Pursuit
- Linear Discriminant Analysis (LDA)
- Mixture Discriminant Analysis (MDA)
- Quadratic Discriminant Analysis (QDA)
- Flexible Discriminant Analysis (FDA)


** Ensemble Algorithms
Ensemble methdos are models composed of multiple *weaker models* that are independenlty trained and whose predictions are combined in some way to make the overall prediction. This is a powerful class of techniques.
- Boosting
- Boostrapped Aggregation (Bagging)
- AdaBoost
- Weighted Average (Blending)
- Stacked Generalization (Stacking)
- Gradient Boosting Machines (GBM)
- Gradient Boosted Regression Trees (GBRT)
- Random Forest

** Other Machine Learning Algorithms

- Feature selection algorithms
- Algorithm accuracy evaluation
- Peformance measures
- Optimzation algorithms

* Further Reading on Machine Learning Algorithms

** Other lists of machine learning algorithms

- [[https://en.wikipedia.org/wiki/List_of_machine_learning_algorithms][Wikepdia list of machine learning algorithms]] - extensive but not useful presentation
- [[https://en.wikipedia.org/wiki/Category:Machine_learning_algorithms][Wikipedia categorized list of machine learning algorithms]]
- [[http://cran.r-project.org/web/views/MachineLearning.html][CRAN Task View: Machine Learning & Statistical Learning]] - list of all packages and supported algorithms
- [[https://amzn.to/30U9Wlh][Top-10 Algorithms in Data Mining]]


** How to study machine learning algorithms
- [[https://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/][How to Learn Any Machine Learning Algorithm]] - Systematic approach that helps to understand any algorithm using /algorithm decription templates/
- [[https://machinelearningmastery.com/create-lists-of-machine-learning-algorithms/][How to create targeted lists of machine learning algorithms]] - help create own systematic lists of machine learning algorithms to jump start work on your next machine learning problem
- [[https://machinelearningmastery.com/how-to-research-a-machine-learning-algorithm/][How to reseach a machine learning algorithms]] - systematic approach to research machine learning algorithms
- [[https://machinelearningmastery.com/how-to-investigate-machine-learning-algorithm-behavior/][How to investigate machine learning algorithm behavior]] - a methodoliogy to understand how machine learning algorithms work by creating and executing very small studies into their behavior
- [[https://machinelearningmastery.com/how-to-implement-a-machine-learning-algorithm/][How to implement a machine learning algorithm]] - A process and tips/tricks for implementing machine learning algorithms from scratch

**  How to run machine learning algorithms
- [[https://machinelearningmastery.com/how-to-get-started-with-machine-learning-algorithms-in-r/][How to get started with machine learning algorithms in R]]
- [[https://machinelearningmastery.com/get-your-hands-dirty-with-scikit-learn-now/][Machine learning algorithm recipes in scikit-learn]]
- [[https://machinelearningmastery.com/how-to-run-your-first-classifier-in-weka/][How to run your first classifier in Weka]]
