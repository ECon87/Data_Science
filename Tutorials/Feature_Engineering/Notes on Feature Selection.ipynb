{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca43844-9356-4c75-bf42-76403cd2c310",
   "metadata": {},
   "source": [
    "# Notes on Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46044e1b-aa3c-4f0d-94eb-264ca67fa9d5",
   "metadata": {},
   "source": [
    "- [Machine Learning Mastery: Feature Selection for Machine Learning](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n",
    "- [Machine Learning Mastery: How to Choose a Feature Selection Method for Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)\n",
    "- [Sklearn Feature Selection article](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n",
    "- [TowardsDataScience: Finding the best features](https://towardsdatascience.com/the-art-of-finding-the-best-features-for-machine-learning-a9074e2ca60d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264b07c-2300-40c4-acdd-7c09bfa859d2",
   "metadata": {},
   "source": [
    "## Introduction on Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b557e-e296-4877-bf1d-74b67922de19",
   "metadata": {},
   "source": [
    "Feature selection allows a researcher to automatically select features that contribute most to the prediction in which they are interested. Irrelevant features can decrease the accuracy of many models, especially linear algorithms such as linear and logistic regressions.\n",
    "\n",
    "Three benefits of performing feature selection before modeling the data are:\n",
    "1. **Reduction of Overfitting**: Less redundant data means less opportunity to make decisions based on noise.\n",
    "2. **Improvement of Accuracy**: Less misleading data means modeling accuracy improves.\n",
    "3. **Reduction of Traing Time**: Less data means that algorithms train faster.\n",
    "\n",
    "In this tutorial, we consider 4 feature selection recipes for ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d5a596-edb4-4b10-bcf0-22e581e7d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'\n",
    "colnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "data = pd.read_csv(filename, names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43aff5b8-b878-4e69-9c1e-1274ae3f37da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5cced5-88e4-41db-b3f8-ae768223d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   preg    768 non-null    int64  \n",
      " 1   plas    768 non-null    int64  \n",
      " 2   pres    768 non-null    int64  \n",
      " 3   skin    768 non-null    int64  \n",
      " 4   test    768 non-null    int64  \n",
      " 5   mass    768 non-null    float64\n",
      " 6   pedi    768 non-null    float64\n",
      " 7   age     768 non-null    int64  \n",
      " 8   class   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230a9e6-be72-41cd-ad3d-76f422d95095",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Univariate Selection\n",
    "\n",
    "Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "Different statistical tests can be used with this selection method. For example, the ANOVA F-value method is appropriate for numerical inputs and categorical data (as in the dataset in our example). This can be used via the `f_classify()` function.\n",
    "\n",
    "In the example below, we select the 4 attributes with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aebc2daa-58b3-4a02-95c0-1239cd037aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
      "[[  6.  148.   33.6  50. ]\n",
      " [  1.   85.   26.6  31. ]\n",
      " [  8.  183.   23.3  32. ]\n",
      " [  1.   89.   28.1  21. ]\n",
      " [  0.  137.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection with Univariate Statistical Tests\n",
    "\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load data\n",
    "X = data.iloc[:, :8]\n",
    "y = data.iloc[:, 8]\n",
    "\n",
    "# Feature Extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, y)\n",
    "\n",
    "# Summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# Summarize selected features\n",
    "print(features[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c38e9d-d109-450e-811f-f6b38e056af2",
   "metadata": {},
   "source": [
    "### 2. Recursive Feature Elimination\n",
    "\n",
    "RFE recursively removes attributes and builds a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "The next example, we use RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as logn as it is skillful and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4b5aeb-d0e0-4877-acde-e6d7243f9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 3\n",
      "Selected features: [ True False False False False  True  True False]\n",
      "Feature ranking: [1 2 4 6 5 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "X = data.iloc[:, :8]\n",
    "y = data.iloc[:, 8]\n",
    "\n",
    "# Feature extraction\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=3000)\n",
    "rfe = RFE(model, n_features_to_select=3)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "print(f'Num features: {fit.n_features_}')\n",
    "print(f'Selected features: {fit.support_}')\n",
    "print(f'Feature ranking: {fit.ranking_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82984ee8-9bfd-4560-9cbe-4ce250ca3bb0",
   "metadata": {},
   "source": [
    "### 3. Principal Component Analysis\n",
    "\n",
    "PCA uses linear algebra to transform the dataset into a compressed form. PCA is a data reduction technique.\n",
    "\n",
    "A property of PCA is that you can choose the number of dimensions or principal components in the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed88a7d9-8dad-4123-983c-5081680e3711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance [0.88854663 0.06159078 0.02579012 0.01308614 0.00744094]\n",
      "[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02\n",
      "   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]\n",
      " [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02\n",
      "   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]\n",
      " [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01\n",
      "   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]\n",
      " [-4.90459604e-02  1.19830016e-01 -2.62742788e-01  8.84369380e-01\n",
      "  -6.55503615e-02  1.92801728e-01  2.69908637e-03 -3.01024330e-01]\n",
      " [ 1.51612874e-01 -8.79407680e-02 -2.32165009e-01  2.59973487e-01\n",
      "  -1.72312241e-04  2.14744823e-02  1.64080684e-03  9.20504903e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load data\n",
    "X = data.iloc[:, :8]\n",
    "y = data.iloc[:, 8]\n",
    "\n",
    "# Feature extraction\n",
    "pca = PCA(n_components=5)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# Summarize components\n",
    "print(f'Explained Variance {fit.explained_variance_ratio_}')\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2612c-4d62-4ca1-9e9a-96fa5637b13b",
   "metadata": {},
   "source": [
    "### 4. Feature Importance\n",
    "\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ffef795-8bc3-4fb6-a54d-eba3e64c99bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10781201 0.2363128  0.09790084 0.07782284 0.0726687  0.13750356\n",
      " 0.12681048 0.14316875]\n"
     ]
    }
   ],
   "source": [
    "# Feature importance with Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Load data\n",
    "X = data.iloc[:, :8]\n",
    "y = data.iloc[:, 8]\n",
    "\n",
    "# Feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc84f2c-df6b-4d88-8017-636d4e232598",
   "metadata": {},
   "source": [
    "## How to Choose a Feature Selection Method For Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad02d5-cd22-4ddf-a845-42a73ec7d2c7",
   "metadata": {},
   "source": [
    "Feature Selection is the process of reducing the number of input variables when developing a predictive model. This allows to reduce the computational cost of modeling and, in some cases, improve the performance of the model.\n",
    "\n",
    "\n",
    "There are two main types of feature selection techniques:\n",
    "1. Supervised, and\n",
    "2. Unsupervised.\n",
    " \n",
    "Supervised methods may be divided into *wrapper, filter and intrinsic*.\n",
    "\n",
    "*Filter-based feature selection methods* used statistical measures to score the correlation or dependence between the input variables that can be filtered to choose the most relevant features. Statistical measures for feature selection must be carefully chosen based on the data type of the input variable or response variable.\n",
    "\n",
    "*Statistical-based feature selection methods* involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable. These methods can be fast and effective, although the choice of statistical measures depends on the data type of both the input and the output variables.\n",
    "\n",
    "Unsupervised methods do not use the target variable, and it is related to dimensionality reduction -- the main difference is that feature selection chooses featuers to keep or remove from the dataset, whereas dimensionality reduction creates a projection of the data resulting in new input features. As such, dimensionality reduction is an alternative to feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1014d-00e2-4cac-908c-6cba48c09cde",
   "metadata": {},
   "source": [
    "This tutorial is divided into 4 parts:\n",
    "1. Feature selection methods\n",
    "2. Statistics for filter feature selection methods\n",
    "    1. Numerical input, numerical output\n",
    "    2. Numerical input, categorical output\n",
    "    3. Categorical input, numerical output\n",
    "    4. Categorical input, categorical output\n",
    "3. Tips and trics for feature selection\n",
    "    1. Correlation statistics\n",
    "    2. Selection method\n",
    "    3. Transform variables\n",
    "    4. What is the best method?\n",
    "4. Worked examples\n",
    "    1. Regression feature selection\n",
    "    2. Classification feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e1b44-2a04-4ca1-8000-c730f0ca0ce7",
   "metadata": {},
   "source": [
    "### 1. Feature Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea066a80-129d-4d4c-8977-2ce8d7711dbf",
   "metadata": {},
   "source": [
    "- Feature Selection: Select a subset of input features from the dataset\n",
    "    - Unsupservised: Do not use the target variable (e.g., remove redundant variables)\n",
    "        - Correlation\n",
    "    - Supervised: Use the target variable (e.g., remove irrelevant variables)\n",
    "        - Wrapper: Search for well-performing subsets of features (create different models with different subsets of input features and select those features that result in the best preforming model according a metric)\n",
    "            - RFE\n",
    "        - Filter: Select subsets of features based on their relationship with the target\n",
    "            - Statistical methods\n",
    "            - Feature importance methods\n",
    "        - Intrinsic: Algorithms that perform automatic feature selection during training\n",
    "            - Decision trees\n",
    "- Dimensionality Reduction: Project input data into a lower-dimensional feature space.      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
